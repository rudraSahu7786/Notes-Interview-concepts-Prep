# Jupyter Notebook: LangChain + Fine-Tuning Notes

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain & Fine-Tuning Notes\n",
    "A concise Jupyter notebook for interview reference and learning." 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChain Overview\n",
    "- Framework to build applications with LLMs.\n",
    "- Connects LLMs with **external data, tools, and memory**.\n",
    "- Use cases: chatbots, RAG, multi-step workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Components\n",
    "### LLMs\n",
    "- Base models (OpenAI, HuggingFace).\n",
    "- Can wrap with LangChain features like caching, streaming.\n",
    "\n",
    "### Prompt Templates\n",
    "```python
    from langchain import PromptTemplate
    template = 'Translate {text} to {language}'
    prompt = PromptTemplate(input_variables=['text','language'], template=template)
    ```\n",
    "- Parameterize prompts efficiently.\n",
    "\n",
    "### Chains\n",
    "- Sequence of actions using LLMs or tools.\n",
    "- Types: LLMChain, SequentialChain, SimpleSequentialChain.\n",
    "\n",
    "### Agents\n",
    "- LLM that can **decide actions dynamically** using tools.\n",
    "- Types: Zero-Shot, ReAct, Few-Shot.\n",
    "\n",
    "### Memory\n",
    "- Stores multi-turn context.\n",
    "- Types: ConversationBufferMemory, ConversationSummaryMemory, CombinedMemory.\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation)\n",
    "- Use embeddings + vector stores to retrieve relevant context for LLM.\n",
    "```python
    from langchain.vectorstores import FAISS
    vector_store = FAISS.from_texts(docs, embeddings)
    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common Interview Q&A\n",
    "\n",
    "**Q1: Difference between Chain and Agent?**\n",
    "- Chain: deterministic sequence of steps.\n",
    "- Agent: dynamic decision-making with tools.\n",
    "\n",
    "**Q2: How to handle multi-turn conversation?**\n",
    "- Use ConversationBufferMemory or ConversationSummaryMemory to maintain context.\n",
    "\n",
    "**Q3: How to implement RAG?**\n",
    "1. Convert documents to embeddings.\n",
    "2. Store in vector DB.\n",
    "3. Retrieve relevant context for query.\n",
    "4. Feed context to LLM for answer.\n",
    "\n",
    "**Q4: How to connect external tools?**\n",
    "- Use Agents + Tools (API calls, Python functions, search engines).\n",
    "\n",
    "**Q5: How to monitor usage/cost?**\n",
    "- Use callbacks and logging in LangChain to track tokens, latency, and API usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning Overview\n",
    "- Specializing a pre-trained LLM for domain-specific tasks.\n",
    "\n",
    "### Types\n",
    "1. Full Fine-Tuning: Update all model weights, expensive.\n",
    "2. LoRA/Adapter Tuning: Update small additional weights, efficient.\n",
    "3. PEFT: Parameter-efficient fine-tuning.\n",
    "4. Prompt/Instruction Tuning: Tune prompts instead of weights, highly efficient.\n",
    "\n",
    "### Steps\n",
    "1. Collect data.\n",
    "2. Preprocess (tokenization, cleaning).\n",
    "3. Train (choose full/LoRA/prompt tuning).\n",
    "4. Evaluate.\n",
    "5. Deploy.\n",
    "\n",
    "### Use Cases\n",
    "- Domain-specific chatbots.\n",
    "- Code generation.\n",
    "- Sentiment classification.\n",
    "- Summarization.\n",
    "\n",
    "### Interview Tips\n",
    "- Know difference between full fine-tuning, LoRA, prompt tuning.\n",
    "- Explain why fine-tuning is needed.\n",
    "- Discuss trade-offs: compute, dataset size, performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Notes\n",
    "- Be ready to explain Python examples for LangChain.\n",
    "- Understand trade-offs: token usage, latency, cost.\n",
    "- Keep examples concise and practical for interviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
